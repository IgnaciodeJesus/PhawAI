{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PhawAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I tried with multiple methods to get the most acurrate predictions for the f1-score\n",
        "# Tried with NN, Random Forest, Adaboost and XGBoost\n",
        "# The best results were obtained with XGBoost, ADASYN and MinMaxScaler in the last section of this file\n",
        "# The section is named \"## Improved XGBOOST model with Hyperparameter Tuning\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First try XGboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install required libraries for running the whole .ipynb file without issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "k9wKQfccEE_f",
        "outputId": "bc2faf1e-b78f-49e3-9a9f-1028459a0d5f"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn==1.3.0 pandas numpy matplotlib seaborn xgboost imblearn optuna torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preprocessing and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z70lGW-tEEz2",
        "outputId": "41f884b4-e9e8-413a-892b-037eec2529db"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test_public = pd.read_csv(\"test_public.csv\")\n",
        "test_private = pd.read_csv(\"test_private.csv\")\n",
        "\n",
        "test_public_ids = test_public[\"ID\"].copy()\n",
        "test_private_ids = test_private[\"ID\"].copy()\n",
        "\n",
        "print(\"Dataset shapes:\")\n",
        "print(\"Train:\", train.shape)\n",
        "print(\"Test Public:\", test_public.shape)\n",
        "print(\"Test Private:\", test_private.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Won't consider columns that are completely null\n",
        "train_null_cols = [col for col in train.columns if train[col].isna().all()]\n",
        "print(\"Columns in train that are completely null:\", train_null_cols)\n",
        "if train_null_cols:\n",
        "    train.drop(columns=train_null_cols, inplace=True)\n",
        "    print(\"Dropped columns\", train_null_cols)\n",
        "print(\"Updated training set shape after deleting columns:\", train.shape)\n",
        "\n",
        "# Removing rows with missing values\n",
        "train.dropna(inplace=True)\n",
        "print(\"Updated training set shape after deleting rows with null values:\", train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training set info:\")\n",
        "print(train.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Head rows\")\n",
        "print(train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation matrix and Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R220KPenDEHh",
        "outputId": "afc97934-06a1-4023-8c1d-9e1f2a9004ec"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print shapes\n",
        "print(\"Training:\", train.shape)\n",
        "print(\"Test Public:\", test_public.shape)\n",
        "print(\"Test Private:\", test_private.shape)\n",
        "\n",
        "# Use only numerical columns for corr matrix\n",
        "numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "corr_matrix = train[numeric_cols].corr()\n",
        "print(\"Correlation matrix table\")\n",
        "print(corr_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False)\n",
        "plt.title(\"Correlation Matrix Graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation with target variable defined below \"CHD_OR_MI\"\n",
        "if \"CHD_OR_MI\" in corr_matrix.columns:\n",
        "    target_corr = corr_matrix[\"CHD_OR_MI\"].sort_values(ascending=False)\n",
        "    print(\"Correlation with target variable:\")\n",
        "    print(target_corr)\n",
        "else:\n",
        "    print(\"Target column Missing\")\n",
        "\n",
        "threshold = 0.05 # Modify this value as needed to improve feature selection and model performance for F1 score\n",
        "if \"CHD_OR_MI\" in corr_matrix.columns:\n",
        "    selected_cols = target_corr[abs(target_corr) >= threshold].index.tolist()\n",
        "    print(f\"Features with absolute correlation greater or equal than {threshold}:\")\n",
        "    print(selected_cols)\n",
        "    train = train[selected_cols]\n",
        "    print(\"Updated Train Shape\", train.shape)\n",
        "    print(\"Remaining columns\", list(train.columns))\n",
        "else:\n",
        "    print(\"Missing Target Column\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_columns = list(train.columns)\n",
        "\n",
        "test_public_aligned_cols = [col for col in test_public.columns if col in train_columns]\n",
        "test_private_aligned_cols = [col for col in test_private.columns if col in train_columns]\n",
        "\n",
        "test_public = test_public[test_public_aligned_cols]\n",
        "test_private = test_private[test_private_aligned_cols]\n",
        "\n",
        "print(\"Test Public shape\", test_public.shape)\n",
        "print(\"Test Private shape\", test_private.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(test_public.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(test_private.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Normalization with MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVDUii7PDD6Y",
        "outputId": "efef978b-092f-4aaa-ca78-8ff48014b237"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "print(\"Train shape before MinMaxScaler:\", train.shape)\n",
        "print(\"Test Public shape before MinMaxScaler:\", test_public.shape)\n",
        "print(\"Test Private shape before MinMaxScaler:\", test_private.shape)\n",
        "\n",
        "exclude_cols = [\"CHD_OR_MI\", \"ID\"] # Excluding specific columns from normalization (target and id are excluded since they're not relevant for the normalization process)\n",
        "numeric_cols_to_normalize = [col for col in train.select_dtypes(include=[np.number]).columns if col not in exclude_cols]\n",
        "\n",
        "print(\"Numeric columns to be normalized:\", numeric_cols_to_normalize)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "train[numeric_cols_to_normalize] = scaler.fit_transform(train[numeric_cols_to_normalize])\n",
        "\n",
        "if len(numeric_cols_to_normalize) > 0:\n",
        "    if set(numeric_cols_to_normalize).issubset(set(test_public.columns)):\n",
        "        test_public[numeric_cols_to_normalize] = scaler.transform(test_public[numeric_cols_to_normalize])\n",
        "    if set(numeric_cols_to_normalize).issubset(set(test_private.columns)):\n",
        "        test_private[numeric_cols_to_normalize] = scaler.transform(test_private[numeric_cols_to_normalize])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Train after normalization:\", train.shape)\n",
        "print(\"Public after normalization:\", test_public.shape)\n",
        "print(\"Private after normalization:\", test_private.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Normalized Training ds\")\n",
        "print(train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the Model and Handling imbalaced data with ADASYN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QE5X6qthDD3b",
        "outputId": "c70663e8-69a1-4e9a-da53-d4d2490eef2b"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "X = train.drop(columns=[\"CHD_OR_MI\", \"ID\"], errors=\"ignore\")\n",
        "y = train[\"CHD_OR_MI\"]\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "# 80/20 split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"y_val shape:\", y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Handling class imbalance with ADASYN\")\n",
        "adasyn = ADASYN(sampling_strategy=\"minority\", random_state=42, n_neighbors=5)\n",
        "X_train_balanced, y_train_balanced = adasyn.fit_resample(X_train, y_train)\n",
        "print(\"X_train_balanced shape:\", X_train_balanced.shape)\n",
        "print(\"y_train_balanced shape:\", y_train_balanced.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training XGBoost\")\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "y_val_pred = xgb_model.predict(X_val)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "conf_mat = confusion_matrix(y_val, y_val_pred)\n",
        "print(\"Confusion Matrix:\", conf_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 4))\n",
        "plt.imshow(conf_mat, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix Validation Set\")\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xticks([0, 1], [\"No\", \"Yes\"])\n",
        "plt.yticks([0, 1], [\"No\", \"Yes\"])\n",
        "\n",
        "for i in range(conf_mat.shape[0]):\n",
        "    for j in range(conf_mat.shape[1]):\n",
        "        plt.text(j, i, str(conf_mat[i, j]), \n",
        "                 ha=\"center\", va=\"center\", \n",
        "                 color=\"white\" if conf_mat[i, j] > conf_mat.max() / 2 else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uNnyzBe7DDxo",
        "outputId": "808d5250-8722-4179-ac5c-08f1266e9441"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "selected_columns = X.columns\n",
        "X_full = train[selected_columns].copy()\n",
        "y_full = train[\"CHD_OR_MI\"].copy()\n",
        "\n",
        "print(\"X_full shape:\", X_full.shape)\n",
        "print(\"y_full shape:\", y_full.shape)\n",
        "\n",
        "adasyn_full = ADASYN(sampling_strategy=\"minority\", random_state=42, n_neighbors=5) # Modify n_neighbors as needed to improve F1 score\n",
        "X_full_balanced, y_full_balanced = adasyn_full.fit_resample(X_full, y_full)\n",
        "\n",
        "print(\"X_full_balanced shape:\", X_full_balanced.shape)\n",
        "print(\"y_full_balanced shape:\", y_full_balanced.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save preditcions to csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g0hB4gGDDut",
        "outputId": "0f689c6a-905b-4acc-c727-5249e0807b20"
      },
      "outputs": [],
      "source": [
        "X_test_public = test_public[selected_columns].copy()  # Align test_public with the selected columns\n",
        "X_test_private = test_private[selected_columns].copy()  # Align test_private with the selected columns\n",
        "\n",
        "# Predictions\n",
        "y_test_public_pred = xgb_model.predict(X_test_public)\n",
        "y_test_private_pred = xgb_model.predict(X_test_private)\n",
        "\n",
        "# New df\n",
        "final_predictions = pd.DataFrame({\n",
        "    \"ID\": pd.concat([test_public_ids, test_private_ids], ignore_index=True),\n",
        "    \"CHD_OR_MI\": np.concatenate([y_test_public_pred, y_test_private_pred])\n",
        "})\n",
        "\n",
        "# Save to csv\n",
        "final_predictions.to_csv(\"resultados.csv\", index=False)\n",
        "print(\"Predictions saved to 'resultados.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2kQI-tZDDrl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Improved XGBOOST model with Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preprocessing and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LFtU-QhL0Ej"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test_public = pd.read_csv(\"test_public.csv\")\n",
        "test_private = pd.read_csv(\"test_private.csv\")\n",
        "\n",
        "test_public_ids = test_public[\"ID\"].copy()\n",
        "test_private_ids = test_private[\"ID\"].copy()\n",
        "\n",
        "print(\"Dataset shapes:\")\n",
        "print(\"Train:\", train.shape)\n",
        "print(\"Test Public:\", test_public.shape)\n",
        "print(\"Test Private:\", test_private.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Won't consider columns that are completely null\n",
        "train_null_cols = [col for col in train.columns if train[col].isna().all()]\n",
        "print(\"Columns in train that are completely null:\", train_null_cols)\n",
        "if train_null_cols:\n",
        "    train.drop(columns=train_null_cols, inplace=True)\n",
        "    print(\"Dropped columns\", train_null_cols)\n",
        "print(\"Updated training set shape after deleting columns:\", train.shape)\n",
        "\n",
        "# Removing rows with missing values\n",
        "train.dropna(inplace=True)\n",
        "print(\"Updated training set shape after deleting rows with null values:\", train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training set info:\")\n",
        "print(train.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3pIiy2ULzzL"
      },
      "outputs": [],
      "source": [
        "print(\"Head rows\")\n",
        "print(train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation matrix and Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print shapes\n",
        "print(\"Training:\", train.shape)\n",
        "print(\"Test Public:\", test_public.shape)\n",
        "print(\"Test Private:\", test_private.shape)\n",
        "\n",
        "# Use only numerical columns for corr matrix\n",
        "numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "corr_matrix = train[numeric_cols].corr()\n",
        "print(\"(ノಠ益ಠ)ノ彡┻━┻\")\n",
        "print(\"Correlation matrix table\")\n",
        "print(corr_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False)\n",
        "plt.title(\"Correlation Matrix Graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation with target variable defined below \"CHD_OR_MI\"\n",
        "if \"CHD_OR_MI\" in corr_matrix.columns:\n",
        "    target_corr = corr_matrix[\"CHD_OR_MI\"].sort_values(ascending=False)\n",
        "    print(\"Correlation with target variable:\")\n",
        "    print(target_corr)\n",
        "else:\n",
        "    print(\"Target column Missing\")\n",
        "\n",
        "threshold = 0.0001 # Modified to include all features\n",
        "if \"CHD_OR_MI\" in corr_matrix.columns:\n",
        "    selected_cols = target_corr[abs(target_corr) >= threshold].index.tolist()\n",
        "    print(f\"Features with absolute correlation greater or equal than {threshold}:\")\n",
        "    print(selected_cols)\n",
        "    train = train[selected_cols]\n",
        "    print(\"Updated Train Shape\", train.shape)\n",
        "    print(\"Remaining columns\", list(train.columns))\n",
        "else:\n",
        "    print(\"Missing Target Column\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_columns = list(train.columns)\n",
        "test_public_aligned_cols = [col for col in test_public.columns if col in train_columns]\n",
        "test_private_aligned_cols = [col for col in test_private.columns if col in train_columns]\n",
        "\n",
        "test_public = test_public[test_public_aligned_cols]\n",
        "test_private = test_private[test_private_aligned_cols]\n",
        "\n",
        "print(\"Test Public shape\", test_public.shape)\n",
        "print(\"Test Private shape\", test_private.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(test_public.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(test_private.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Normalization with MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "print(\"Train shape before MinMaxScaler:\", train.shape)\n",
        "print(\"Test Public shape before MinMaxScaler:\", test_public.shape)\n",
        "print(\"Test Private shape before MinMaxScaler:\", test_private.shape)\n",
        "\n",
        "exclude_cols = [\"CHD_OR_MI\", \"ID\"] # Excluding specific columns from normalization (target and id are excluded since they're not relevant for the normalization process)\n",
        "numeric_cols_to_normalize = [\n",
        "    col for col in train.select_dtypes(include=[np.number]).columns\n",
        "    if col not in exclude_cols\n",
        "]\n",
        "\n",
        "print(\"Numeric columns to be normalized:\", numeric_cols_to_normalize)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "train[numeric_cols_to_normalize] = scaler.fit_transform(train[numeric_cols_to_normalize])\n",
        "\n",
        "if len(numeric_cols_to_normalize) > 0:\n",
        "    if set(numeric_cols_to_normalize).issubset(set(test_public.columns)):\n",
        "        test_public[numeric_cols_to_normalize] = scaler.transform(test_public[numeric_cols_to_normalize])\n",
        "    if set(numeric_cols_to_normalize).issubset(set(test_private.columns)):\n",
        "        test_private[numeric_cols_to_normalize] = scaler.transform(test_private[numeric_cols_to_normalize])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpWouKULLzvm"
      },
      "outputs": [],
      "source": [
        "print(\"Train after normalization:\", train.shape)\n",
        "print(\"Public after normalization:\", test_public.shape)\n",
        "print(\"Private after normalization:\", test_private.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Normalized Training ds\")\n",
        "print(train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the Model and Handling imbalaced data with ADASYN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCiiw6LLLzsd"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "X = train.drop(columns=[\"CHD_OR_MI\", \"ID\"], errors=\"ignore\")\n",
        "y = train[\"CHD_OR_MI\"]\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "# 80/20 split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.001, random_state=42, stratify=y # Yea I know, I'm using 0.001 for testing purposes\n",
        ")\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"y_val shape:\", y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"Handling class imbalance with ADASYN\")\n",
        "adasyn = ADASYN(sampling_strategy=\"minority\", random_state=42, n_neighbors=5) # Modify n_neighbors as needed to improve F1 score\n",
        "X_train_balanced, y_train_balanced = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"X_train_balanced shape:\", X_train_balanced.shape)\n",
        "print(\"y_train_balanced shape:\", y_train_balanced.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training the XGBoost model\")\n",
        "xgb_model = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    n_estimators=1000,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.01,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    gamma=1\n",
        ")\n",
        "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "y_val_pred = xgb_model.predict(X_val)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "conf_mat = confusion_matrix(y_val, y_val_pred)\n",
        "print(\"Confusion Matrix\", conf_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 4))\n",
        "plt.imshow(conf_mat, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix Validation Set\")\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xticks([0, 1], [\"No\", \"Yes\"])\n",
        "plt.yticks([0, 1], [\"No\", \"Yes\"])\n",
        "\n",
        "for i in range(conf_mat.shape[0]):\n",
        "    for j in range(conf_mat.shape[1]):\n",
        "        plt.text(j, i, str(conf_mat[i, j]), \n",
        "                 ha=\"center\", va=\"center\", \n",
        "                 color=\"white\" if conf_mat[i, j] > conf_mat.max() / 2 else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip uninstall scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install scikit-learn==1.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_distributions = {\n",
        "    \"n_estimators\": [100, 300, 500, 800, 1000, 1600],\n",
        "    \"max_depth\": [3, 5, 7, 9, 11],\n",
        "    \"learning_rate\": [0.1, 0.05, 0.01, 0.005, 0.0001],\n",
        "    \"subsample\": [0.7, 0.8, 1.0],\n",
        "    \"colsample_bytree\": [0.7, 0.8, 1.0],\n",
        "    \"gamma\": [0, 1, 5, 10]\n",
        "}\n",
        "\n",
        "base_xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=base_xgb,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=30, # Iterations number, modify as needed to improve F1 score\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Randomized Search for best hyperparameters\")\n",
        "random_search.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "best_model = random_search.best_estimator_\n",
        "print(\"Best hyperparameters:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "y_val_pred_best = best_model.predict(X_val)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val, y_val_pred_best))\n",
        "\n",
        "conf_mat_best = confusion_matrix(y_val, y_val_pred_best)\n",
        "print(\"Confusion Matrix:\", conf_mat_best)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.imshow(conf_mat_best, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix Validation Set\")\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xticks([0, 1], [\"No\", \"Yes\"])\n",
        "plt.yticks([0, 1], [\"No\", \"Yes\"])\n",
        "\n",
        "for i in range(conf_mat.shape[0]):\n",
        "    for j in range(conf_mat.shape[1]):\n",
        "        plt.text(j, i, str(conf_mat[i, j]), \n",
        "                 ha=\"center\", va=\"center\", \n",
        "                 color=\"white\" if conf_mat[i, j] > conf_mat.max() / 2 else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "selected_columns = X.columns\n",
        "X_full = train[selected_columns].copy()\n",
        "y_full = train[\"CHD_OR_MI\"].copy()\n",
        "\n",
        "print(\"X_full shape:\", X_full.shape)\n",
        "print(\"y_full shape:\", y_full.shape)\n",
        "\n",
        "adasyn_full = ADASYN(sampling_strategy=\"minority\", random_state=42, n_neighbors=5) # Modify n_neighbors as needed to improve F1 score\n",
        "X_full_balanced, y_full_balanced = adasyn_full.fit_resample(X_full, y_full)\n",
        "\n",
        "print(\"X_full_balanced shape:\", X_full_balanced.shape)\n",
        "print(\"y_full_balanced shape:\", y_full_balanced.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42) # Modify n_splits as needed to improve F1 score\n",
        "cv_scores = cross_val_score(best_model, X_full_balanced, y_full_balanced, cv=cv, scoring='f1')\n",
        "print(\"F1 score from cross-validation:\", cv_scores)\n",
        "print(\"Mean F1-score:\", np.mean(cv_scores))\n",
        "\n",
        "print(\"Retraining the best model on the entire balanced dataset\")\n",
        "best_model.fit(X_full_balanced, y_full_balanced)\n",
        "\n",
        "y_full_pred = best_model.predict(X_full)\n",
        "print(\"Classification Report Final:\")\n",
        "print(classification_report(y_full, y_full_pred))\n",
        "\n",
        "final_cm = confusion_matrix(y_full, y_full_pred)\n",
        "print(\"Confusion Matrix Final:\", final_cm)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(final_cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix Tuned\")\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xticks([0, 1], [\"No\", \"Yes\"])\n",
        "plt.yticks([0, 1], [\"No\", \"Yes\"])\n",
        "plt.tight_layout()\n",
        "\n",
        "for i in range(conf_mat.shape[0]):\n",
        "    for j in range(conf_mat.shape[1]):\n",
        "        plt.text(j, i, str(conf_mat[i, j]), \n",
        "                 ha=\"center\", va=\"center\", \n",
        "                 color=\"white\" if conf_mat[i, j] > conf_mat.max() / 2 else \"black\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save predictions to csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "X_test_public = test_public[selected_columns].copy()\n",
        "X_test_private = test_private[selected_columns].copy()\n",
        "\n",
        "y_test_public_pred = best_model.predict(X_test_public)\n",
        "y_test_private_pred = best_model.predict(X_test_private)\n",
        "\n",
        "final_predictions = pd.DataFrame({\n",
        "    \"ID\": pd.concat([test_public_ids, test_private_ids], ignore_index=True),\n",
        "    \"CHD_OR_MI\": np.concatenate([y_test_public_pred, y_test_private_pred])\n",
        "})\n",
        "\n",
        "# Ensure the format is correct\n",
        "print(final_predictions.head())\n",
        "\n",
        "# Save to csv\n",
        "final_predictions.to_csv(\"resultadosXGBoost8.csv\", index=False)\n",
        "print(\"Final predictions saved to 'resultadosXGBoost7.csv'.\") # Renamed it manually to resultados.csv"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
